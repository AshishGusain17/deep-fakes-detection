{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embed_trained_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrngHwYMg2v4"
      },
      "source": [
        "from os.path import join, exists\n",
        "from os import listdir, makedirs\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import h5py\n",
        "import cv2\n",
        "\n",
        "from keras.layers.pooling import GlobalAveragePooling2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras import applications, Sequential, optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Flatten, Dense, concatenate,  Dropout\n",
        "from keras.optimizers import Adam, Nadam\n",
        "from keras.utils import plot_model\n",
        "\n",
        "## required for semi-hard triplet loss:\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBLuu-rFgJNU",
        "outputId": "32430233-e5ff-479b-f697-2591497b7f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrFilJmxgjq3"
      },
      "source": [
        "!cp -r --recursive \"/content/drive/My Drive/train160.h5\" \"/content\"\n",
        "!cp -r --recursive \"/content/drive/My Drive/test160.h5\" \"/content\"\n",
        "!cp -r --recursive \"/content/drive/My Drive/val160.h5\" \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufVj4heggy_n",
        "outputId": "13c084a4-0d1f-4e84-d6cb-17c49ab8d619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "dataset = h5py.File('train160.h5', \"r\")\n",
        "trainX = dataset[\"X\"][:]\n",
        "trainY = dataset[\"Y\"][:]\n",
        "print(trainX.shape, trainY.shape)\n",
        "dataset.close()\n",
        "\n",
        "\n",
        "dataset = h5py.File('test160.h5', \"r\")\n",
        "testX = dataset[\"X\"][:]\n",
        "testY = dataset[\"Y\"][:]\n",
        "print(testX.shape, testY.shape)\n",
        "dataset.close()\n",
        "\n",
        "\n",
        "dataset = h5py.File('val160.h5', \"r\")\n",
        "valX = dataset[\"X\"][:]\n",
        "valY = dataset[\"Y\"][:]\n",
        "print(valX.shape, valY.shape)\n",
        "dataset.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(227190, 512) (227190, 1)\n",
            "(28000, 512) (28000, 1)\n",
            "(20000, 512) (20000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2gChEvaLOhK"
      },
      "source": [
        "def pairwise_distance(feature, squared=False):\n",
        "\t\"\"\"Computes the pairwise distance matrix with numerical stability.\n",
        "\n",
        "\toutput[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
        "\n",
        "\tArgs:\n",
        "\t  feature: 2-D Tensor of size [number of data, feature dimension].\n",
        "\t  squared: Boolean, whether or not to square the pairwise distances.\n",
        "\n",
        "\tReturns:\n",
        "\t  pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
        "\t\"\"\"\n",
        "\tpairwise_distances_squared = math_ops.add(\n",
        "\t\tmath_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
        "\t\tmath_ops.reduce_sum(\n",
        "\t\t\tmath_ops.square(array_ops.transpose(feature)),\n",
        "\t\t\taxis=[0],\n",
        "\t\t\tkeepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tarray_ops.transpose(feature))\n",
        "\n",
        "\t# Deal with numerical inaccuracies. Set small negatives to zero.\n",
        "\tpairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
        "\t# Get the mask where the zero distances are at.\n",
        "\terror_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
        "\n",
        "\t# Optionally take the sqrt.\n",
        "\tif squared:\n",
        "\t\tpairwise_distances = pairwise_distances_squared\n",
        "\telse:\n",
        "\t\tpairwise_distances = math_ops.sqrt(\n",
        "\t\t\tpairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
        "\n",
        "\t# Undo conditionally adding 1e-16.\n",
        "\tpairwise_distances = math_ops.multiply(\n",
        "\t\tpairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
        "\n",
        "\tnum_data = array_ops.shape(feature)[0]\n",
        "\t# Explicitly set diagonals to zero.\n",
        "\tmask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
        "\t\tarray_ops.ones([num_data]))\n",
        "\tpairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
        "\treturn pairwise_distances\n",
        "\n",
        "\n",
        "def masked_maximum(data, mask, dim=1):\n",
        "\t\"\"\"Computes the axis wise maximum over chosen elements.\n",
        "\n",
        "\tArgs:\n",
        "\t  data: 2-D float `Tensor` of size [n, m].\n",
        "\t  mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "\t  dim: The dimension over which to compute the maximum.\n",
        "\n",
        "\tReturns:\n",
        "\t  masked_maximums: N-D `Tensor`.\n",
        "\t\tThe maximized dimension is of size 1 after the operation.\n",
        "\t\"\"\"\n",
        "\taxis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
        "\tmasked_maximums = math_ops.reduce_max(\n",
        "\t\tmath_ops.multiply(data - axis_minimums, mask), dim,\n",
        "\t\tkeepdims=True) + axis_minimums\n",
        "\treturn masked_maximums\n",
        "\n",
        "\n",
        "def masked_minimum(data, mask, dim=1):\n",
        "\t\"\"\"Computes the axis wise minimum over chosen elements.\n",
        "\n",
        "\tArgs:\n",
        "\t  data: 2-D float `Tensor` of size [n, m].\n",
        "\t  mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "\t  dim: The dimension over which to compute the minimum.\n",
        "\n",
        "\tReturns:\n",
        "\t  masked_minimums: N-D `Tensor`.\n",
        "\t\tThe minimized dimension is of size 1 after the operation.\n",
        "\t\"\"\"\n",
        "\taxis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
        "\tmasked_minimums = math_ops.reduce_min(\n",
        "\t\tmath_ops.multiply(data - axis_maximums, mask), dim,\n",
        "\t\tkeepdims=True) + axis_maximums\n",
        "\treturn masked_minimums\n",
        "\n",
        "\n",
        "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
        "\tdel y_true\n",
        "\tmargin = 1.\n",
        "\tlabels = y_pred[:, :1]\n",
        "\n",
        " \n",
        "\tlabels = tf.cast(labels, dtype='int32')\n",
        "\n",
        "\tembeddings = y_pred[:, 1:]\n",
        "\n",
        "\t### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
        "\t\n",
        "\t# Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
        "\t# lshape=array_ops.shape(labels)\n",
        "\t# assert lshape.shape == 1\n",
        "\t# labels = array_ops.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "\t# Build pairwise squared distance matrix.\n",
        "\tpdist_matrix = pairwise_distance(embeddings, squared=True)\n",
        "\t# Build pairwise binary adjacency matrix.\n",
        "\tadjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
        "\t# Invert so we can select negatives only.\n",
        "\tadjacency_not = math_ops.logical_not(adjacency)\n",
        "\n",
        "\t# global batch_size  \n",
        "\tbatch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
        "\n",
        "\t# Compute the mask.\n",
        "\tpdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
        "\tmask = math_ops.logical_and(\n",
        "\t\tarray_ops.tile(adjacency_not, [batch_size, 1]),\n",
        "\t\tmath_ops.greater(\n",
        "\t\t\tpdist_matrix_tile, array_ops.reshape(\n",
        "\t\t\t\tarray_ops.transpose(pdist_matrix), [-1, 1])))\n",
        "\tmask_final = array_ops.reshape(\n",
        "\t\tmath_ops.greater(\n",
        "\t\t\tmath_ops.reduce_sum(\n",
        "\t\t\t\tmath_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
        "\t\t\t0.0), [batch_size, batch_size])\n",
        "\tmask_final = array_ops.transpose(mask_final)\n",
        "\n",
        "\tadjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
        "\tmask = math_ops.cast(mask, dtype=dtypes.float32)\n",
        "\n",
        "\t# negatives_outside: smallest D_an where D_an > D_ap.\n",
        "\tnegatives_outside = array_ops.reshape(\n",
        "\t\tmasked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
        "\tnegatives_outside = array_ops.transpose(negatives_outside)\n",
        "\n",
        "\t# negatives_inside: largest D_an.\n",
        "\tnegatives_inside = array_ops.tile(\n",
        "\t\tmasked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
        "\tsemi_hard_negatives = array_ops.where(\n",
        "\t\tmask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "\tloss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "\n",
        "\tmask_positives = math_ops.cast(\n",
        "\t\tadjacency, dtype=dtypes.float32) - array_ops.diag(\n",
        "\t\tarray_ops.ones([batch_size]))\n",
        "\n",
        "\t# In lifted-struct, the authors multiply 0.5 for upper triangular\n",
        "\t#   in semihard, they take all positive pairs except the diagonal.\n",
        "\tnum_positives = math_ops.reduce_sum(mask_positives)\n",
        "\n",
        "\tsemi_hard_triplet_loss_distance = math_ops.truediv(\n",
        "\t\tmath_ops.reduce_sum(\n",
        "\t\t\tmath_ops.maximum(\n",
        "\t\t\t\tmath_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
        "\t\tnum_positives,\n",
        "\t\tname='triplet_semihard_loss')\n",
        "\t\n",
        "\t### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
        "\treturn semi_hard_triplet_loss_distance\n",
        "\n",
        "\n",
        "def triplets_loss(y_true, y_pred):\n",
        "\t\n",
        "#     embeddings = K.cast(embeddings, 'float32')\n",
        "#     with sess.as_default():\n",
        "#         print(embeddings.eval())\n",
        "\t\n",
        "\tembeddings = y_pred\n",
        "\tanchor_positive = embeddings[:10]\n",
        "\tnegative = embeddings[10:]\n",
        "#     print(anchor_positive)\n",
        "\n",
        "\t# Compute pairwise distance between all of anchor-positive\n",
        "\tdot_product = K.dot(anchor_positive, K.transpose(anchor_positive))\n",
        "\tsquare = K.square(anchor_positive)\n",
        "\ta_p_distance = K.reshape(K.sum(square, axis=1), (-1,1)) - 2.*dot_product  + K.sum(K.transpose(square), axis=0) + 1e-6\n",
        "\ta_p_distance = K.maximum(a_p_distance, 0.0) ## Numerical stability\n",
        "#     with K.get_session().as_default():\n",
        "#         print(a_p_distance.eval())\n",
        "#     print(\"Pairwise shape: \", a_p_distance)\n",
        "#     print(\"Negative shape: \", negative)\n",
        "\n",
        "\t# Compute distance between anchor and negative\n",
        "\tdot_product_2 = K.dot(anchor_positive, K.transpose(negative))\n",
        "\tnegative_square = K.square(negative)\n",
        "\ta_n_distance = K.reshape(K.sum(square, axis=1), (-1,1)) - 2.*dot_product_2  + K.sum(K.transpose(negative_square), axis=0)  + 1e-6\n",
        "\ta_n_distance = K.maximum(a_n_distance, 0.0) ## Numerical stability\n",
        "\t\n",
        "\thard_negative = K.reshape(K.min(a_n_distance, axis=1), (-1, 1))\n",
        "\t\n",
        "\tdistance = (a_p_distance - hard_negative + 0.2)\n",
        "\tloss = K.mean(K.maximum(distance, 0.0))/(2.)\n",
        "\n",
        "#     with K.get_session().as_default():\n",
        "#             print(loss.eval())\n",
        "\t\t\t\n",
        "\treturn loss\n",
        "\n",
        "\n",
        "def create_base_network(image_input_shape, embedding_size):\n",
        "    input_image = Input(shape = image_input_shape)\n",
        "    x = Flatten()(input_image)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(embedding_size)(x)\n",
        "\n",
        "    base_network = Model(inputs=input_image, outputs=x)\n",
        "    plot_model(base_network, to_file='base_network.png', show_shapes=True, show_layer_names=True)\n",
        "    return base_network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3h1fKurLR_z",
        "outputId": "350a182b-81a4-4a22-c423-17e971d613f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 100\n",
        "input_image_shape = (512)\n",
        "embedding_size = 64\n",
        "\n",
        "base_network = create_base_network(input_image_shape, embedding_size)\n",
        "base_network.summary()\n",
        "\n",
        "\n",
        "input_images = Input(shape=input_image_shape, name='input_image')  \n",
        "input_labels = Input(shape=(1,), name='input_label')  \n",
        "embeddings = base_network([input_images])       \n",
        "labels_plus_embeddings = concatenate([input_labels, embeddings]) \n",
        "model = Model(inputs=[input_images, input_labels], outputs=labels_plus_embeddings)\n",
        "plot_model(model, to_file='end_model.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=3e-4)\n",
        "model.compile(loss=triplet_loss_adapted_from_tf, optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 512)]             0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 435,136\n",
            "Trainable params: 435,136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_label (InputLayer)        [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "functional_1 (Functional)       (None, 64)           435136      input_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 65)           0           input_label[0][0]                \n",
            "                                                                 functional_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 435,136\n",
            "Trainable params: 435,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcNQpoffhKOW"
      },
      "source": [
        "dummy_gt_train = np.zeros((len(trainX), embedding_size + 1))\n",
        "dummy_gt_val = np.zeros((len(valX), embedding_size + 1))\n",
        "\n",
        "H = model.fit(\n",
        "    x=[trainX,trainY],\n",
        "    y=dummy_gt_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([valX, valY], dummy_gt_val),\n",
        "    verbose=2\n",
        "    )\n",
        "model.save(\"triplets.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uHEiJJouTwM"
      },
      "source": [
        "model = load_model(\"triplets.hdf5\", custom_objects={'triplet_loss_adapted_from_tf':triplet_loss_adapted_from_tf})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptCqqzqwq30E"
      },
      "source": [
        "input_image_shape = (512)\n",
        "embedding_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4FswNppGa8",
        "outputId": "c047bc70-6b92-4d4e-92af-9319406af710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import copy\n",
        "X = copy.deepcopy(trainX)\n",
        "Y = copy.deepcopy(trainY).reshape(len(trainY),)\n",
        "tX = copy.deepcopy(testX)\n",
        "tY = copy.deepcopy(testY).reshape(len(testY),)\n",
        "print(X.shape, Y.shape, tX.shape, tY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(227190, 512) (227190,) (28000, 512) (28000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeTqWAfavKSx"
      },
      "source": [
        "sgd = linear_model.SGDClassifier()\n",
        "sgd.fit(X, Y)\n",
        "acc = accuracy_score(tY, sgd.predict(tX))\n",
        "print(\"Accuracy with 512 embeddings as features for sgd: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJtHoKCK4XQ5"
      },
      "source": [
        "test_model = create_base_network(input_image_shape,embedding_size=embedding_size)\n",
        "for layer_target, layer_source in zip(test_model.layers,  model.layers[2].layers):\n",
        "    weights = layer_source.get_weights()\n",
        "    layer_target.set_weights(weights)\n",
        "\n",
        "X_64 = test_model.predict(X)\n",
        "print(X_64.shape)\n",
        "tX_64 = test_model.predict(tX)\n",
        "print(tX_64.shape)\n",
        "\n",
        "sgd = linear_model.SGDClassifier()\n",
        "sgd.fit(X_64, Y)\n",
        "acc = accuracy_score(tY, sgd.predict(tX_64))\n",
        "print(\"Accuracy with 64 embeddings as features with weighted model: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVY-Wi3y6AJt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bziseE06AHQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqi0KHLG67zr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8FO-f3673N"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators= 150)\n",
        "rfc.fit(X, Y)\n",
        "acc = accuracy_score(tY, rfc.predict(tX))\n",
        "print(\"Accuracy with 512 embeddings as features for random classifier: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz7DXly2sQde"
      },
      "source": [
        "test_model = create_base_network(input_image_shape,embedding_size=embedding_size)\n",
        "for layer_target, layer_source in zip(test_model.layers,  model.layers[2].layers):\n",
        "    weights = layer_source.get_weights()\n",
        "    layer_target.set_weights(weights)\n",
        "\n",
        "X_64 = test_model.predict(X)\n",
        "print(X_64.shape)\n",
        "tX_64 = test_model.predict(tX)\n",
        "print(tX_64.shape)\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators= 80)\n",
        "rfc.fit(X_64, Y)\n",
        "acc = accuracy_score(tY, rfc.predict(tX_64))\n",
        "print(\"Accuracy with 64 embeddings as features with weighted model: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lB1UPx46ABg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh6K2ylp5_-y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWvq9tywu3Jv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA49rplLu3G1"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X, Y)\n",
        "acc = accuracy_score(tY, lr.predict(tX))\n",
        "print(\"Accuracy with 512 embeddings as features for logistic classifier: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63mdVskvZUT"
      },
      "source": [
        "test_model = create_base_network(input_image_shape,embedding_size=embedding_size)\n",
        "for layer_target, layer_source in zip(test_model.layers,  model.layers[2].layers):\n",
        "    weights = layer_source.get_weights()\n",
        "    layer_target.set_weights(weights)\n",
        "\n",
        "X_64 = test_model.predict(X)\n",
        "print(X_64.shape)\n",
        "tX_64 = test_model.predict(tX)\n",
        "print(tX_64.shape)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_64, Y)\n",
        "acc = accuracy_score(tY, lr.predict(tX_64))\n",
        "print(\"Accuracy with 64 embeddings as features with weighted model: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bUT4poEwCn8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xL91c2RwCzB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuFMC2bCwC3g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD3hOuupwDGR"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=8)\n",
        "knn.fit(X, Y)\n",
        "acc = accuracy_score(tY, knn.predict(tX))\n",
        "print(\"Accuracy with 512 embeddings as features for K nearest neighbours: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGQ-Rsj8wC_e"
      },
      "source": [
        "test_model = create_base_network(input_image_shape,embedding_size=embedding_size)\n",
        "for layer_target, layer_source in zip(test_model.layers,  model.layers[2].layers):\n",
        "    weights = layer_source.get_weights()\n",
        "    layer_target.set_weights(weights)\n",
        "\n",
        "X_64 = test_model.predict(X)\n",
        "print(X_64.shape)\n",
        "tX_64 = test_model.predict(tX)\n",
        "print(tX_64.shape)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=8)\n",
        "knn.fit(X_64, Y)\n",
        "acc = accuracy_score(tY, knn.predict(tX_64))\n",
        "print(\"Accuracy with 64 embeddings as features with weighted model: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJO7d2lrwxP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QVu7LadwxVI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe4YPoXWwxN7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pBXLviXwxKl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkvY-RNEwxIZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BJE_POpgltZ"
      },
      "source": [
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X, Y)\n",
        "acc = accuracy_score(tY, dtc.predict(tX))\n",
        "print(\"Accuracy with 512 embeddings as features for decision tree: \",acc)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmMaFsowv_4"
      },
      "source": [
        "test_model = create_base_network(input_image_shape,embedding_size=embedding_size)\n",
        "for layer_target, layer_source in zip(test_model.layers,  model.layers[2].layers):\n",
        "    weights = layer_source.get_weights()\n",
        "    layer_target.set_weights(weights)\n",
        "\n",
        "X_64 = test_model.predict(X)\n",
        "print(X_64.shape)\n",
        "tX_64 = test_model.predict(tX)\n",
        "print(tX_64.shape)\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_64, Y)\n",
        "acc = accuracy_score(tY, dtc.predict(tX_64))\n",
        "print(\"Accuracy with 64 embeddings as features with weighted model: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txNah16jwv8-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}