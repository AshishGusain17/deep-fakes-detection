{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_embed.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z6nIyhmBBBA",
        "outputId": "a75eddfd-9196-4c94-8978-ff3faa98c136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!pip install keras_facenet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_facenet\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/c5/6fadf919a86c44b87ba9d8134cc83820b8fa8a98f5c68ff676179e052839/keras-facenet-0.3.2.tar.gz\n",
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn->keras_facenet) (2.4.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn->keras_facenet) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn->keras_facenet) (1.15.0)\n",
            "Building wheels for collected packages: keras-facenet\n",
            "  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-cp36-none-any.whl size=10387 sha256=ec9bf2f4d5b21accdf0bddb9386c76f13cd868731dafad118c53a22134df9c0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/53/9a/36c4b52fd22faf4f710d5047d874655b85a1b2cf77accfb9bd\n",
            "Successfully built keras-facenet\n",
            "Installing collected packages: mtcnn, keras-facenet\n",
            "Successfully installed keras-facenet-0.3.2 mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7QAypKd1_sa"
      },
      "source": [
        "from os.path import join, exists\n",
        "from os import listdir, makedirs\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import h5py\n",
        "import cv2\n",
        "\n",
        "from keras_facenet import FaceNet\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras import applications, Sequential, optimizers\n",
        "from keras.layers import Flatten, Dropout, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skE2Ii-uiDMy",
        "outputId": "ac38ce70-909e-476f-fdc7-3aaa0ab71ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-lKybekiUef"
      },
      "source": [
        "!cp -r --recursive \"/content/drive/My Drive/data.zip\" \"/content\"\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiO4nm49roV6"
      },
      "source": [
        "trainX , testX , valX = [] , [] , []\n",
        "trainY , testY , valY = [] , [] , []\n",
        "counter = 0\n",
        "facenet = FaceNet()\n",
        "\n",
        "for dataset in [\"train\",\"test\",\"val\"]:\n",
        "    for ind,label in enumerate([\"manipulated_sequences\",\"original_sequences\"]):\n",
        "        data_path = os.path.join(\"data\",dataset,label)\n",
        "        for img_name in os.listdir(data_path):\n",
        "            img_path = os.path.join(data_path, img_name)\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            # print(img.shape)\n",
        "\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "            # print(img.shape)\n",
        "\n",
        "            embeddings = facenet.embeddings(img)\n",
        "            # print(embeddings.shape)\n",
        "\n",
        "            if dataset == \"train\":\n",
        "                trainX.append(np.transpose(embeddings).flatten())\n",
        "                trainY.append(ind)\n",
        "            elif dataset == \"test\":\n",
        "                testX.append(np.transpose(embeddings).flatten())\n",
        "                testY.append(ind)\n",
        "            else:\n",
        "                valX.append(np.transpose(embeddings).flatten())\n",
        "                valY.append(ind)\n",
        "            \n",
        "            counter += 1 \n",
        "\n",
        "            sys.stdout.write('\\r')\n",
        "            sys.stdout.write(str(counter))\n",
        "            sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME1A0_c49nty"
      },
      "source": [
        "trainX = np.array(trainX)\n",
        "trainY = np.array(trainY)\n",
        "trainY = trainY.reshape((len(trainY),1))\n",
        "\n",
        "testX = np.array(testX)\n",
        "testY = np.array(testY)\n",
        "testY = testY.reshape((len(testY),1))\n",
        "\n",
        "valX = np.array(valX)\n",
        "valY = np.array(valY)\n",
        "valY = valY.reshape((len(valY),1))\n",
        "\n",
        "\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)\n",
        "print(valX.shape)\n",
        "print(valY.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRAJQYc5Fve"
      },
      "source": [
        "archive = h5py.File('train160.h5', 'w')\n",
        "archive.create_dataset('/X', data = trainX)\n",
        "archive.create_dataset('/Y',data = trainY)\n",
        "archive.close()\n",
        "\n",
        "archive = h5py.File('test160.h5', 'w')\n",
        "archive.create_dataset('/X', data = testX)\n",
        "archive.create_dataset('/Y',data = testY)\n",
        "archive.close()\n",
        "\n",
        "archive = h5py.File('val160.h5', 'w')\n",
        "archive.create_dataset('/X', data = valX)\n",
        "archive.create_dataset('/Y',data = valY)\n",
        "archive.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBO050VY9jTG"
      },
      "source": [
        "!cp -r --recursive  \"train160.h5\"        \"/content/drive/My Drive\"\n",
        "!cp -r --recursive  \"test160.h5\"        \"/content/drive/My Drive\"\n",
        "!cp -r --recursive  \"val160.h5\"        \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfnpijO96LWt"
      },
      "source": [
        "dataset = h5py.File('train160.h5', \"r\")\n",
        "x = dataset[\"X\"][:]\n",
        "y = dataset[\"Y\"][:]\n",
        "print(x.shape,y.shape)\n",
        "dataset.close()\n",
        "\n",
        "\n",
        "dataset = h5py.File('test160.h5', \"r\")\n",
        "x = dataset[\"X\"][:]\n",
        "y = dataset[\"Y\"][:]\n",
        "print(x.shape,y.shape)\n",
        "dataset.close()\n",
        "\n",
        "\n",
        "dataset = h5py.File('val160.h5', \"r\")\n",
        "x = dataset[\"X\"][:]\n",
        "y = dataset[\"Y\"][:]\n",
        "print(x.shape,y.shape)\n",
        "dataset.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaqxw2gl8GkE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}